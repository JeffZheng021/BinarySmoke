{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33KXBBvxosLd"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbdPMO_ibusx",
    "outputId": "043235a1-7b52-4ecc-c343-f0c6d522cc7e"
   },
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5tcgh6jocAs",
    "outputId": "6f8dfecb-5a3b-448f-cfed-e2612788812e"
   },
   "outputs": [],
   "source": [
    "# All necessary packages\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"gauravduttakiit/smoker-status-prediction-using-biosignals\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EYlAWfyoujg",
    "outputId": "10e41a01-4931-4764-c674-03005a3e502d"
   },
   "outputs": [],
   "source": [
    "#Since no target variable avaliable in the testing dataset in kaggle\n",
    "#Training dataset has already enough entrie, we can do a train/test split using the training dataset only for this project\n",
    "df= pd.read_csv(path + \"/train_dataset.csv\")\n",
    "\n",
    "#Showcasing the dataset\n",
    "#df.head()\n",
    "\n",
    "# Printing information and descriptive statistics for df\n",
    "print(\"Training Data Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nTraining Data Descriptive Statistics:\")\n",
    "print(df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSV--sGrOE-"
   },
   "source": [
    "As we can see the taget variable is a binary result consists of 0 and 1 called \"smoking\", with all other variables already cleaned being in either dummies or numerical values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0llgXszu3Fb"
   },
   "source": [
    "# Understanding & Showcasing Features (For Proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO0aAWzps_yg",
    "outputId": "8dc5df09-8686-41ef-de43-282f2d142d54"
   },
   "outputs": [],
   "source": [
    "# Distrubution of key features:\n",
    "# Set the style\n",
    "#\n",
    "# Plot histograms for age, weight, cholesterol\n",
    "df[['age', 'weight(kg)', 'Cholesterol', 'smoking']].hist(bins=15, figsize=(15, 10), layout=(1, 4), color='skyblue', edgecolor='black')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Boxplot Distrubution of key features:\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "sns.boxplot(ax=axes[0], y=df['age'], color='lightblue').set_title('Age Distribution')\n",
    "sns.boxplot(ax=axes[1], y=df['weight(kg)'], color='lightgreen').set_title('Weight Distribution')\n",
    "sns.boxplot(ax=axes[2], y=df['Cholesterol'], color='salmon').set_title('Cholesterol Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Mean Cholesterol by Age group:\n",
    "# Define the age bins and labels for the categorical grouping\n",
    "bins = [20, 32, 45, 58, 71, 85]\n",
    "labels = ['20-32', '33-45', '46-58', '59-71', '72-85']\n",
    "\n",
    "# Create a new column 'age_group' using `pd.cut` to categorize 'age' into bins\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Calculate the mean cholesterol for each age group\n",
    "mean_cholesterol = df.groupby('age_group')['Cholesterol'].mean()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "mean_cholesterol.plot(kind='bar', color='skyblue')\n",
    "plt.title('Mean Cholesterol by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Mean Cholesterol')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion matrix for age, weight, cholesterol, and smoking:\n",
    "# Selecting important features\n",
    "relevant_data = df[['age', 'weight(kg)', 'Cholesterol', 'smoking']]\n",
    "\n",
    "# Compute the correlation matrix for the selected variables\n",
    "corr = relevant_data.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", center=0)\n",
    "plt.title('Correlation Matrix for Selected Variables')\n",
    "plt.show()\n",
    "\n",
    "# Pairwise correlation between feature variables:\n",
    "sns.pairplot(df[['age', 'weight(kg)', 'Cholesterol', 'systolic', 'smoking']], hue='smoking')\n",
    "plt.title('Pairwise Relationships')\n",
    "plt.show()\n",
    "\n",
    "# Distributioin of smoking status:\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "colors = ['skyblue', 'orange']\n",
    "df['smoking'].value_counts().plot.pie(\n",
    "    explode=[0, 0.1],\n",
    "    autopct='%1.1f%%',\n",
    "    shadow=True,\n",
    "    colors=colors\n",
    ")\n",
    "ax.set_title('Smoking Status Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds4vse7hu_fZ"
   },
   "source": [
    "# Train, Val, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70_anC-9u8sz"
   },
   "outputs": [],
   "source": [
    "X = df.drop('smoking', axis=1)\n",
    "#X = df.drop(['smoking', 'age_group'], axis=1)\n",
    "\n",
    "y = df['smoking']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=88)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4HWOqR9SgJc"
   },
   "source": [
    "# Baseline Model for Feature Selection and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L9ZN-Db2Sg8-",
    "outputId": "319189b2-b080-4e64-b5b1-e0c08addc7cf"
   },
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Check coefficients (feature importance)\n",
    "coefficients = pd.Series(log_reg.coef_[0], index=X_train.columns)\n",
    "coefficients.sort_values(ascending=False).plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Logistic Regression Coefficients (Log Odds)')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix for feature analysis\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SzsnzLOQ8ma"
   },
   "source": [
    "As we see from the very last row of the correlation graph. There are a couple of the features that is fairly useless: hearing(left&right), cholesterol, LDL, Urine protein. these all contribute to less than 0.05 than absolute value, which in our opinion would be considered not significantly contributing to the prediction of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeM6Xz-vu0XU"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels\n",
    "bins = [20, 32, 45, 58, 71, 85]\n",
    "labels = ['20-32', '33-45', '46-58', '59-71', '72-85']\n",
    "\n",
    "# Filter data within the bins\n",
    "df = df[df['age'].between(bins[0], bins[-1])]\n",
    "\n",
    "# Create the 'age_group' column (temporary)\n",
    "age_group = pd.cut(df['age'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Create dummy variables for 'age_group'\n",
    "age_group_dummies = pd.get_dummies(age_group, prefix='age_group', drop_first=True)\n",
    "\n",
    "# Ensure dummy variables are integers (1/0)\n",
    "age_group_dummies = age_group_dummies.astype(int)\n",
    "\n",
    "# Add dummy variables to the DataFrame (without keeping 'age_group')\n",
    "df = pd.concat([df, age_group_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_yYlRg0Q8R0"
   },
   "outputs": [],
   "source": [
    "# Drop un-needed columns\n",
    "X = df.drop(['smoking','hearing(left)','hearing(right)','Cholesterol','LDL', 'Urine protein'], axis=1)\n",
    "\n",
    "y = df['smoking']\n",
    "\n",
    "# Train/Test split for the after feature selected data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=88)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDoKAHVBdQ-0"
   },
   "source": [
    "# Pretuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjZf_GG53Qi1"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85ziSKE1T8Ft",
    "outputId": "f52e65ad-33c4-41e8-b8d4-728ab215956d"
   },
   "outputs": [],
   "source": [
    "# Define pipeline for scaling and model fitting\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=10000, tol=1e-6))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Print best parameters and scores\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Truc9U-93S4a"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvZjJJF7_wLp",
    "outputId": "26891889-8a00-40c4-8e9d-eb018207ca80"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Random search for hyperparameter tuning\n",
    "random_search_RF = RandomizedSearchCV(RandomForestClassifier(random_state=88), param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=2, random_state=88, scoring='roc_auc')\n",
    "random_search_RF.fit(X_val, y_val)\n",
    "\n",
    "# Print best parameters and scores\n",
    "print(\"\\nRandom Forest Pre-tuning:\\n\")\n",
    "print(\"Best Parameters:\", random_search_RF.best_params_)\n",
    "print(\"Best Score:\", random_search_RF.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNxe7lzV4b2M"
   },
   "source": [
    "## Support Vector Machine (LinearSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioDV5e-9yLnO",
    "outputId": "52b71b27-4ee9-4f04-9e46-8ec8cec04f17"
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),        # Step 1: Scale features\n",
    "    ('model', LinearSVC(random_state=88))  # Step 2: Fit Linear SVM\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__loss': ['squared_hinge'],\n",
    "    'model__dual': [False],\n",
    "    'model__tol': [1e-3, 1e-4],\n",
    "    'model__max_iter': [10000, 20000]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search_SVM = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    "    random_state=88,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Starting RandomizedSearchCV for Linear SVM...\")\n",
    "random_search_SVM.fit(X_val, y_val)\n",
    "\n",
    "# Output results\n",
    "print(\"\\nLinear SVM Pre-tuning Results:\\n\")\n",
    "print(\"Best Parameters:\", random_search_SVM.best_params_)\n",
    "print(\"Best Score (AUC-ROC):\", random_search_SVM.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3ZDJRDL4fRV"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwSP8CuzuGcm"
   },
   "outputs": [],
   "source": [
    "# Step 1: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SC399Gm1dLaE",
    "outputId": "33b1bfac-23fe-4ca9-917d-451913e0401b"
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=2, dropout_rate=0.5):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.7)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 50)\n",
    "\n",
    "    # Create model\n",
    "    input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "    model = SimpleNN(input_size=input_size, num_classes=2, dropout_rate=dropout_rate)\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_train.values, dtype=torch.long)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(y_val.values, dtype=torch.long)\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best Parameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afl9PJl8vGXi"
   },
   "source": [
    "# Fitting each of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Myt9HUB13KbP"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7U6XPd0LruCU",
    "outputId": "94b30b07-b7c1-4320-e773-e795b8fa9cea"
   },
   "outputs": [],
   "source": [
    "# Train the model to its full potential using the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Set Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot ROC curve\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_prob):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (LR)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (LR)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRkagzt3Cl-r"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w9_BMS6yUNz1",
    "outputId": "410c0dbc-acab-4548-f319-e15accb9c383"
   },
   "outputs": [],
   "source": [
    "# Train the model to its full potential using the best parameters\n",
    "best_params_RF = random_search_RF.best_params_\n",
    "\n",
    "best_model_RF = RandomForestClassifier(\n",
    "    n_estimators=best_params_RF['n_estimators'],\n",
    "    min_samples_split=best_params_RF['min_samples_split'],\n",
    "    min_samples_leaf=best_params_RF['min_samples_leaf'],\n",
    "    max_features=best_params_RF['max_features'],\n",
    "    max_depth=best_params_RF['max_depth'],\n",
    "    random_state=88  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Fit the final model on the full training set (X_train, y_train)\n",
    "best_model_RF.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set (X_test, y_test)\n",
    "y_pred = best_model_RF.predict(X_test)\n",
    "y_prob = best_model_RF.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy)\n",
    "print(\"Test Set ROC AUC Score:\", roc_auc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (RF)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(best_model_RF, X_test, y_test, cmap='Blues')\n",
    "plt.title('Confusion Matrix (RF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JughhUUACpeH"
   },
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JD0HivQfCrc8",
    "outputId": "23240df3-7e35-4139-f5ca-bd09b09698f3"
   },
   "outputs": [],
   "source": [
    "# Extract best parameters from RandomizedSearchCV\n",
    "best_params_SVM = random_search_SVM.best_params_\n",
    "\n",
    "# Initialize the best Linear SVM model with the selected hyperparameters\n",
    "best_model_SVM = LinearSVC(\n",
    "    C=best_params_SVM['model__C'],\n",
    "    penalty=best_params_SVM['model__penalty'],\n",
    "    loss=best_params_SVM['model__loss'],\n",
    "    dual=best_params_SVM['model__dual'],\n",
    "    tol=best_params_SVM['model__tol'],\n",
    "    max_iter=best_params_SVM['model__max_iter'],\n",
    "    random_state=88  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Scale the training and test data (as LinearSVC requires scaled features)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the final Linear SVM model on the full training set\n",
    "best_model_SVM.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_decision = best_model_SVM.decision_function(X_test_scaled)  # Get decision scores for ROC-AUC\n",
    "y_pred = best_model_SVM.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_decision)\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy)\n",
    "print(\"Test Set ROC AUC Score:\", roc_auc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_decision)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'Linear SVM (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Linear SVM)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(best_model_SVM, X_test_scaled, y_test, cmap='Blues')\n",
    "plt.title('Confusion Matrix (Linear SVM)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGBcQoBI7uxe"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cRcmmko_7Gp6",
    "outputId": "c0c61b15-44b6-4ae9-e100-78bdca89b175"
   },
   "outputs": [],
   "source": [
    "# Get the best parameters from the Optuna study\n",
    "best_params_NN = study.best_params\n",
    "\n",
    "# Train the final model using the best parameters\n",
    "final_model_NN = SimpleNN(input_size=X_train.shape[1], num_classes=2)\n",
    "final_model_NN.dropout = best_params_NN['dropout_rate']\n",
    "\n",
    "# DataLoader setup with the best batch size\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params_NN['batch_size'], shuffle=True)\n",
    "\n",
    "# Loss and optimizer with the best learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model_NN.parameters(), lr=best_params_NN['learning_rate'])\n",
    "\n",
    "# Training loop with the best number of epochs\n",
    "final_model_NN.train()\n",
    "for epoch in range(best_params_NN['num_epochs']):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model_NN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params_NN['batch_size'], shuffle=False)\n",
    "final_model_NN.eval()\n",
    "\n",
    "# Collect predictions and probabilities\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = final_model_NN(inputs)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Get probability for positive class\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        y_prob.extend(probabilities.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert y_test to numpy for compatibility\n",
    "y_test_np = y_test.values\n",
    "\n",
    "# Display the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_np, y_pred))\n",
    "\n",
    "# Calculate and plot the ROC curve\n",
    "roc_auc = roc_auc_score(y_test_np, y_prob)\n",
    "fpr, tpr, _ = roc_curve(y_test_np, y_prob)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (NN)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "cm = confusion_matrix(y_test_np, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix (NN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NjEBh43vKgl"
   },
   "source": [
    "# Results and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK5NclqMmmkg"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yH15VFlPml3F",
    "outputId": "5594e93b-08c9-4223-f5c1-360744e7293f"
   },
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer for Logistic Regression\n",
    "logistic_model = best_model.named_steps['model']\n",
    "\n",
    "explainer = shap.LinearExplainer(logistic_model, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "\n",
    "X_test_array = X_test_scaled\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test_array, feature_names=X_train.columns)\n",
    "\n",
    "# Force plot for individual predictions\n",
    "shap.initjs()\n",
    "\n",
    "idx = 88  # Example index\n",
    "\n",
    "print('Test Document ID: ', idx)\n",
    "print('True Label: ', y_test.iloc[idx] if isinstance(y_test, pd.Series) else y_test[idx])\n",
    "print('Pred Label: ', y_pred[idx])\n",
    "print('Correct Prediction' if y_test.iloc[idx] == y_pred[idx] else 'Incorrect Prediction')\n",
    "display(shap.force_plot(\n",
    "    explainer.expected_value, shap_values[idx, :], X_test_array[idx, :],\n",
    "    feature_names=X_train.columns\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeFtqJmJmpyb"
   },
   "source": [
    "## Random Forest (Take too long due to Random Forest Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "9UKr8Zi_msST",
    "outputId": "08d511d3-47f5-48af-9681-d29b299dc4d0"
   },
   "outputs": [],
   "source": [
    "# Extract feature importances from the trained Random Forest model\n",
    "feature_importances = best_model_RF.feature_importances_\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "sorted_features = np.array(X_train.columns)[sorted_idx]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_features, feature_importances[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance (Gini Importance)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance for Random Forest\")\n",
    "plt.gca().invert_yaxis()  # Flip y-axis for descending order\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "ttO9SL-14SnI",
    "outputId": "56fbe6dc-10ce-4ff0-d7a4-affc6cb8ee5b"
   },
   "outputs": [],
   "source": [
    "# Select an individual tree from the Random Forest\n",
    "tree = best_model_RF.estimators_[0]  # First tree in the forest\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree,\n",
    "          filled=True,\n",
    "          feature_names=X_train.columns,\n",
    "          class_names=['Non Smoking', 'Smoking'],\n",
    "          rounded=True)\n",
    "plt.title(\"Decision Tree from Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zIQStypmj6Z"
   },
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "loZbLxvMvRL-",
    "outputId": "9f737637-58ee-4a03-fdf0-7ccc9914425d"
   },
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer for Linear SVM\n",
    "explainer = shap.LinearExplainer(best_model_SVM, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Convert scaled test data to an array if needed\n",
    "X_test_array = X_test_scaled\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test_array, feature_names=X_train.columns)\n",
    "\n",
    "# Force plot for individual predictions\n",
    "shap.initjs()\n",
    "\n",
    "idx = 88\n",
    "\n",
    "print('Test Document ID: ', idx)\n",
    "print('True Label: ', y_test.iloc[idx])\n",
    "print('Pred Label: ', y_pred[idx])\n",
    "print('Correct Prediction' if y_test.iloc[idx] == y_pred[idx] else 'Incorrect Prediction')\n",
    "display(shap.force_plot(\n",
    "    explainer.expected_value, shap_values[idx, :], X_test_array[idx, :],\n",
    "    feature_names=X_train.columns\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_rxyYP1r4WY"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "747930732e5d4ec4acd8da334f978bad",
      "9307885e3159452c8d61e6fd8ecad2b7",
      "ec471cbfb49c4168b71f251fcfd1e15f",
      "a500697c0ec64aeca2dc84ac46dd0f66",
      "97b9231c3c43419d8cf4fa50f46ddd43",
      "78e04078b9a94cc0bc7bdb8d1e0084dd",
      "128494f7641a4678ba821841a10e6370",
      "2d5e3550ceeb4fe5a3ca96f975a6405c",
      "2a0af1b0d5544395aa29fe37004c7191",
      "ba350e3909d1428185db16d56708000c",
      "bc2734c9081e4eda9e215b0342d7bf86"
     ]
    },
    "id": "zmash3fc0hCk",
    "outputId": "6ef0b894-fb1b-4057-ea45-4fb95e05d04e"
   },
   "outputs": [],
   "source": [
    "# Function for prediction\n",
    "def model_predict(X):\n",
    "    # Convert NumPy array to PyTorch tensor\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    final_model_NN.eval()\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = final_model_NN(X_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Probability for the positive class\n",
    "    return probabilities.numpy()\n",
    "\n",
    "# Reduce background size\n",
    "background = shap.sample(X_train_scaled, 200, random_state=88)\n",
    "\n",
    "# Reduce test size\n",
    "X_test_sample = shap.sample(X_test_scaled, 200, random_state=88)\n",
    "\n",
    "# Initialize KernelExplainer\n",
    "explainer = shap.KernelExplainer(model_predict, background)\n",
    "\n",
    "# Compute SHAP values for the smaller test set\n",
    "shap_values = explainer.shap_values(X_test_sample, nsamples=50)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
